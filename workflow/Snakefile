
import numpy as np
import pandas as pd
import os

from glob import glob
from io import StringIO

# load the sample sheet
sumstats = pd.read_csv(config['sumstats'], sep='\t', index_col='ID', header=0)
assert 'PATH' in sumstats.columns, 'Error, missing column "PATH" in sumstats sample sheet.'
assert 'N' in sumstats.columns, 'Error, missing column "N" in sumstats sample sheet.'

# load the big40 metadata
big40 = pd.read_csv('resources/big40_metadata.tsv.gz', sep='\t')
big40['idp'] = ['{:04d}'.format(x) for x in big40['Pheno'].values ]
big40.set_index('idp',drop=False,inplace=True)

# load the big40 phenotypes we wish to analyse
with open(config['big40_phenotypes'], 'r') as infile:
    big40_phenotypes = ['{:04d}'.format(int(x.strip())) for x in infile]

# triggers running all rules
rule all:
    input:
        'results/results.tsv.gz'

#################################
# dependencies / pre-processing #
#################################

rule install_ldsc_conda:
    conda:
        "env/ldsc.yml"
    output:
        touch('ldsc_conda.ok')
    shell:
        "echo 'dummy rule to install ldsc conda environment'"

rule download_hm3_snplist:
    output:
        "resources/w_hm3/w_hm3.snplist"
    shell:
        "cd resources/w_hm3 && "
        "wget https://data.broadinstitute.org/alkesgroup/LDSCORE/w_hm3.snplist.bz2 && "
        "bunzip2 w_hm3.snplist.bz2"

rule download_and_munge_brain_sumstats:
    input:
        "resources/w_hm3/w_hm3.snplist"
    output:
        ss_tmp=temp("resources/sumstats/big40/{idp}.txt.gz"),
        ss_munged="resources/sumstats/big40_munged/{idp}.sumstats.gz"
    params:
        N=lambda wc: big40.loc[wc['idp'],'N(all)'],
        out_prefix=lambda wc, output: output['ss_munged'].replace('.sumstats.gz','')
    conda:
        "env/ldsc.yml"
    resources:
        threads=1,
        mem_mb=8000,
        time="02:00:00"
    shell:
        "wget --no-verbose -O {output[ss_tmp]} https://open.win.ox.ac.uk/ukbiobank/big40/release2/stats33k/{wildcards[idp]}.txt.gz && "
        "zcat {output[ss_tmp]} | awk 'BEGIN{{print \"CHR\", \"SNP\", \"POS\", \"A1\", \"A2\", \"BETA\", \"SE\", \"P\"}}{{if(NR>1){{$NF=10**(-1*$NF); print $0}}}}' > {params[out_prefix]}.reformat.tmp.txt && "
        "python ldsc/munge_sumstats.py --chunksize 500000 --sumstats {params[out_prefix]}.reformat.tmp.txt --N {params[N]} --out {params[out_prefix]} --merge-alleles {input} && "
        "rm {params[out_prefix]}.reformat.tmp.txt"


rule munge_target_sumstats:
    input:
        snplist="resources/w_hm3/w_hm3.snplist",
        ss=lambda wc: sumstats.loc[wc['id'],'PATH']
    output:
        ss_munged='results/munged_sumstats/{id}.sumstats.gz'
    params:
        out_prefix=lambda wc, output: output['ss_munged'].replace('.sumstats.gz',''),
        N=lambda wc: sumstats.loc[wc['id'],'N'],
        pvcol=lambda wc: '--p "{}"'.format(sumstats.loc[wc['id'],'PVAL_COLUMN']) if not sumstats.loc[wc['id'],'PVAL_COLUMN'] == '.' else '',
        a1col=lambda wc: '--a1 "{}"'.format(sumstats.loc[wc['id'],'A1_COLUMN']) if not sumstats.loc[wc['id'],'A1_COLUMN'] == '.' else '',
        a2col=lambda wc: '--a2 "{}"'.format(sumstats.loc[wc['id'],'A2_COLUMN']) if not sumstats.loc[wc['id'],'A2_COLUMN'] == '.' else ''
    conda:
        "env/ldsc.yml"
    resources:
        threads=1,
        mem_mb=8000,
        time="02:00:00"
    shell:
        "python ldsc/munge_sumstats.py --chunksize 500000 --sumstats {input[ss]} --N {params[N]} --out {params[out_prefix]} --merge-alleles {input[snplist]} {params[pvcol]} {params[a1col]} {params[a2col]}"


rule download_ld_scores:
    output:
        directory('resources/eur_w_ld_chr')
    shell:
        "cd resources && "
        "wget --no-verbose https://data.broadinstitute.org/alkesgroup/LDSCORE/eur_w_ld_chr.tar.bz2 && "
        "tar -jxvf eur_w_ld_chr.tar.bz2"


# rule to trigger downloading and preprocessing of all requested big40 phenotypes
rule all_download_and_munge_big40:
    input:
        expand(rules.download_and_munge_brain_sumstats.output, idp=big40_phenotypes)


# rule to trigger preprocessing of all target sumstats
rule all_munge_sumstats:
    input:
        expand(rules.munge_target_sumstats.output, id=sumstats.index.values)

# make rules above local
localrules:
    all_download_and_munge_big40,
    all_munge_sumstats


##########
# S-LDSC #
##########


rule download_baseline_ld:
    output:
        baseline=expand("1000G_EUR_Phase3_baseline/baseline.{chrom}.{suffix}", chrom=range(1,23), suffix=["annot.gz","l2.M", "l2.M_5_50", "l2.ldscore.gz"]),
        weights=expand("weights_hm3_no_hla/weights.{chrom}.l2.ldscore.gz", chrom=range(1,23)),
        ok=touch("download_baseline.ok")
    shell:
        "wget https://data.broadinstitute.org/alkesgroup/LDSCORE/1000G_Phase3_baseline_ldscores.tgz && "
        "wget https://data.broadinstitute.org/alkesgroup/LDSCORE/weights_hm3_no_hla.tgz && "
        "tar -xvzf 1000G_Phase3_baseline_ldscores.tgz && "
        "tar -xvzf weights_hm3_no_hla.tgz "


rule download_ldsc_cts:
    output:
        ok=touch("download_{cts_name}.ok"),
        ldcts="{cts_name}.ldcts"
    shell:
        "wget https://data.broadinstitute.org/alkesgroup/LDSCORE/LDSC_SEG_ldscores/{wildcards[cts_name]}_1000Gv3_ldscores.tgz && "
        "tar -xvzf {wildcards[cts_name]}_1000Gv3_ldscores.tgz "


# make rules above local
localrules:
    download_ldsc_cts,
    download_baseline_ld


rule run_ldsc_cts:
    input:
        sumstats="results/munged_sumstats/{id}.sumstats.gz",
        ldcts="{cts_name}.ldcts"
    output:
        ok=touch('results/ldsc_cts/{id}/{cts_name}.ok')
    conda:
        "env/ldsc.yml"
    resources:
        threads=1,
        mem_mb=8000,
        time="01:00:00"
    shell:
        "ldsc/ldsc.py "
        "--h2-cts {input[sumstats]} "
        "--ref-ld-chr 1000G_EUR_Phase3_baseline/baseline. "
        "--out results/ldsc_cts/{wildcards[id]}/{wildcards[cts_name]} "
        "--ref-ld-chr-cts {input[ldcts]} "
        "--w-ld-chr weights_hm3_no_hla/weights."


################
# running ldsc #
################

# chunk the phenotypes into batches
checkpoint chunk_phenotypes:
    input:
        pheno_file=config['big40_phenotypes']
    output:
        chunks=directory('results/pheno_chunks')
    run:
        os.makedirs(output['chunks'])
        for i, p in enumerate(big40_phenotypes):
            outfile='results/pheno_chunks/{}.txt'.format(int(i/100))
            with open(outfile, 'a+') as out:
                out.write(p+'\n')

# make rule above local
localrules:
    chunk_phenotypes

# input function for rule below
def input_files_from_chunk(wildcards):
    with open('results/pheno_chunks/{}.txt'.format(wildcards['i']),'r') as infile:
        phenos = [ p.strip() for p in infile ]
    input_files = ['resources/sumstats/big40_munged/{}.sumstats.gz'.format(p) for p in phenos]
    return input_files

rule genetic_correlation_chunk:
    input:
        ss_big40=input_files_from_chunk,
        ss_target='results/munged_sumstats/{id}.sumstats.gz',
        ldscores=rules.download_ld_scores.output # this is a directory
    output:
        log='results/genetic_correlation_chunk/{id}/{i}.log'
    params:
        out_prefix=lambda wc, output: output['log'].replace('.log',''),
        ss_string=lambda wc, input: input['ss_target'] + ',' + ','.join(input['ss_big40'])
    conda:
        'env/ldsc.yml'
    shell:
        "python ldsc/ldsc.py "
        "--ref-ld-chr {input[ldscores]}/ "
        "--out {params[out_prefix]} "
        "--rg {params[ss_string]} "
        "--w-ld-chr {input[ldscores]}/"

# input function for rule below
def aggregate_input_chunks(wildcards):
    checkpoint_output = checkpoints.chunk_phenotypes.get(**wildcards).output['chunks']

    # "i" will contain all the chunk numbers
    i = glob_wildcards(os.path.join(checkpoint_output, '{i}.txt'))[0]
    print('aggregating {} chunks'.format(len(i)))

    return expand("results/genetic_correlation_chunk/{id}/{i}.log", id=wildcards.id, i=i)

rule aggregate_chunks:
    input:
        chunks=aggregate_input_chunks
    output:
        touch('results/genetic_correlation_chunk/{id}.ok')
 
rule all_aggregate_chunks:
    input:
        expand(rules.aggregate_chunks.output, id=sumstats.index.values)


#######################
# merging the results #
#######################

# function used in the rule below
def parse_log(path):
    """
    parses output of genetic correlation, returns a pandas DataFrame
    """
    with open(path, 'r') as infile:
        keep=False
        record = ''
        for line in infile:
            if line.startswith('Summary of Genetic Correlation'):
                keep = True
                continue
            if not keep:
                continue
            if len(line) == 1:
                break
            record += line
    return pd.read_csv(StringIO(record), delim_whitespace=True)
    
rule parse_logs:
    input:
        rules.all_aggregate_chunks.input,
        metadata='resources/big40_metadata.tsv.gz'
    output:
        results_tsv='results/results.tsv.gz'
    run:
        metadata = pd.read_csv(input['metadata'], sep='\t')
        logs = glob('results/genetic_correlation_chunk/*/*.log')
        ssid = [ p.split('/')[-2] for p in logs]
        chunk = [ p.split('/')[-1].split('.')[0] for p in logs ]
        genet_cor = [ parse_log(x) for x in logs ]
        for i, _ in enumerate(genet_cor):
            genet_cor[i]['ss'] = ssid[i]
            genet_cor[i]['chunk'] = chunk[i]
        genet_cor = pd.concat(genet_cor)
        genet_cor['big40_id'] = genet_cor.p2.str.split('/',expand=True)[3].str.split('.',expand=True)[0]
        metadata['big40_id'] = ['{:04d}'.format(i) for i in metadata.Pheno]
        genet_cor = genet_cor.merge(metadata[['big40_id','UKB ID','IDP short name','Cat.','Category name','IDP description']], on=['big40_id'], validate='many_to_one').sort_values('p')
        genet_cor.to_csv(output['results_tsv'], sep='\t')
        
